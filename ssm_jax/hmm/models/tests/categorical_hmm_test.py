"""
https://github.com/hmmlearn/hmmlearn/blob/main/lib/hmmlearn/tests/test_categorical_hmm.py
"""
import pytest

import jax.numpy as jnp
import jax.random as jr
from jax import vmap
from jax.tree_util import register_pytree_node_class
from ssm_jax.hmm.models.base import BaseHMM
from ssm_jax.hmm.models.categorical_hmm import CategoricalHMM


def normalize(a, axis=None):
    """
    Normalize the input array so that it sums to 1.
    Parameters
    ----------
    a : array
        Non-normalized input data.
    axis : int
        Dimension along which normalization is performed.
    Notes
    -----
    Modifies the input **inplace**.
    """
    a_sum = a.sum(axis)
    if axis and a.ndim > 1:
        # Make sure we don't divide by zero.
        a_sum[a_sum == 0] = 1
        shape = list(a.shape)
        shape[axis] = 1
        a_sum.shape = shape

    return a / a_sum


def normalized(X, axis=None):
    return normalize(X, axis=axis)


def new_hmm():
    """
    States : rainy, sunny
    Emissions : walk, shop, clean
    """
    num_states = 2
    num_emissions = 1
    num_classes = 3
    initial_probabilities = jnp.array([0.6, 0.4])
    transition_matrix = jnp.array([[0.7, 0.3],
                                   [0.4, 0.6]])
    emission_probs = jnp.array([[0.1, 0.4, 0.5],
                                [0.6, 0.3, 0.1]])
    emission_probs = emission_probs.reshape(num_states, num_emissions, num_classes)
    hmm = CategoricalHMM(initial_probabilities, transition_matrix, emission_probs)
    return hmm


class TestCategoricalAgainstWikipedia:
    """
    Examples from Wikipedia:
    - http://en.wikipedia.org/wiki/Hidden_Markov_model
    - http://en.wikipedia.org/wiki/Viterbi_algorithm
    """

    def test_decode_viterbi(self):
        # From http://en.wikipedia.org/wiki/Viterbi_algorithm:
        # "This reveals that the observations ['walk', 'shop', 'clean']
        #  were most likely generated by states ['Sunny', 'Rainy', 'Rainy'],
        #  with probability 0.01344."
        hmm = new_hmm()
        X = jnp.arange(3).reshape(3, 1)
        state_sequence = vmap(hmm.most_likely_states)(X)
        # assert round(jnp.exp(log_prob), 5) == 0.01344
        assert jnp.allclose(jnp.squeeze(state_sequence), jnp.array([1, 0, 0]))

    def test_predict(self):
        X = jnp.arange(3).reshape(3, 1)
        hmm = new_hmm()
        posteriors = vmap(hmm.filter)(X)
        filtered_probs = jnp.squeeze(posteriors.filtered_probs, axis=1)
        print(filtered_probs)
        assert jnp.allclose(
            filtered_probs,
            jnp.array(
                [
                    [0.23170303, 0.76829697],
                    [0.62406281, 0.37593719],
                    [0.86397706, 0.13602294],
                ]
            ),
            atol=1e-1,
        )


class TestCategoricalHMM:
    def setup(self):
        self.num_states = 2
        self.num_classes = 3

    def test_score_samples(self, key=jr.PRNGKey(0), num_timesteps=10):
        emissions = jr.randint(key, shape=(num_timesteps, 1), minval=0, maxval=self.num_classes - 1)
        hmm = new_hmm()

        posterior = hmm.filter(emissions)
        filtered_probs = posterior.filtered_probs
        predicted_probs = posterior.predicted_probs

        assert filtered_probs.shape == (num_timesteps, self.num_states)
        assert predicted_probs.shape == (num_timesteps, self.num_states)

        assert jnp.allclose(filtered_probs.sum(axis=-1), jnp.ones((num_timesteps, 1)), atol=1e-6, rtol=1e-6)
        assert jnp.allclose(predicted_probs.sum(axis=-1), jnp.ones((num_timesteps, 1)), atol=1e-6, rtol=1e-6)

    def test_sample(self, key=jr.PRNGKey(0), num_timesteps=1000):
        hmm = new_hmm()
        state_sequence, emissions = hmm.sample(key, num_timesteps)
        assert len(emissions) == len(state_sequence) == num_timesteps
        assert len(jnp.unique(emissions)) == self.num_classes

    def test_em(self, key=jr.PRNGKey(0), num_states=4, num_emissions=2, num_classes=3, num_timesteps=1000):
        emis_key, sample_key, init_key = jr.split(key, 3)

        true_initial_probs = jnp.ones((num_states,)) / (num_states * 1.0)
        true_transition_matrix = 0.90 * jnp.eye(num_states) + 0.10 * jnp.ones((num_states, num_states)) / num_states
        true_emission_probs = jr.uniform(emis_key, shape=(num_states, num_emissions, num_classes))
        true_emission_probs /= true_emission_probs.sum(axis=-1, keepdims=True)
        true_hmm = CategoricalHMM(true_initial_probs, true_transition_matrix, true_emission_probs)
        state_sequence, emissions = true_hmm.sample(sample_key, num_timesteps)

        hmm1 = CategoricalHMM.random_initialization(init_key, num_states, num_emissions, num_classes)
        lps1 = hmm1.fit_em(emissions[None, ...])

        # Make a CategoricalHMM that defaults to the BaseHMM EM algorithm
        @register_pytree_node_class
        class DummyCategoricalHMM(CategoricalHMM):
            e_step = BaseHMM.e_step
            m_step = BaseHMM.m_step

        hmm2 = DummyCategoricalHMM.random_initialization(init_key, num_states, num_emissions, num_classes)
        lps2 = hmm2.fit_em(emissions[None, ...])

        assert jnp.allclose(hmm1.emission_probs.value, hmm2.emission_probs.value, atol=1e-1)
        assert jnp.allclose(hmm1.initial_probs.value, hmm2.initial_probs.value, atol=1e-1)
        assert jnp.allclose(lps1, lps2, atol=1e-2 * num_timesteps)
